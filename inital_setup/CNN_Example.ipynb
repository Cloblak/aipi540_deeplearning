{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f8b7960-5b94-4615-ac0e-99c586664801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2683b1d1-bc9f-4e4b-ac13-598b3aec2e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 60, 1, 8)          664       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 58, 1, 8)          200       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 29, 1, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 1, 8)          200       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 1, 8)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 104)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 104)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 105       \n",
      "=================================================================\n",
      "Total params: 1,169\n",
      "Trainable params: 1,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "400/400 [==============================] - 56s 141ms/step - loss: 0.4341 - acc: 0.5692 - f1macro: 0.3838 - val_loss: 0.4632 - val_acc: 0.5375 - val_f1macro: 0.4528\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 51s 128ms/step - loss: 0.3266 - acc: 0.6970 - f1macro: 0.6623 - val_loss: 0.4975 - val_acc: 0.5016 - val_f1macro: 0.4833\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 53s 131ms/step - loss: 0.2584 - acc: 0.7606 - f1macro: 0.7442 - val_loss: 0.4849 - val_acc: 0.5219 - val_f1macro: 0.5101\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 51s 127ms/step - loss: 0.2345 - acc: 0.7784 - f1macro: 0.7638 - val_loss: 0.4771 - val_acc: 0.5250 - val_f1macro: 0.5102\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 50s 126ms/step - loss: 0.2190 - acc: 0.7915 - f1macro: 0.7780 - val_loss: 0.4712 - val_acc: 0.5305 - val_f1macro: 0.5193\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 51s 129ms/step - loss: 0.2070 - acc: 0.8017 - f1macro: 0.7901 - val_loss: 0.5100 - val_acc: 0.4914 - val_f1macro: 0.4762\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 50s 126ms/step - loss: 0.1969 - acc: 0.8111 - f1macro: 0.8010 - val_loss: 0.4939 - val_acc: 0.5039 - val_f1macro: 0.4918\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 49s 123ms/step - loss: 0.1877 - acc: 0.8187 - f1macro: 0.8086 - val_loss: 0.4997 - val_acc: 0.4992 - val_f1macro: 0.4828\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 48s 120ms/step - loss: 0.1836 - acc: 0.8224 - f1macro: 0.8130 - val_loss: 0.4970 - val_acc: 0.5023 - val_f1macro: 0.4729\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 127829s 320s/step - loss: 0.1830 - acc: 0.8223 - f1macro: 0.8132 - val_loss: 0.4711 - val_acc: 0.5320 - val_f1macro: 0.5185\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 52s 131ms/step - loss: 0.1835 - acc: 0.8214 - f1macro: 0.8119 - val_loss: 0.4652 - val_acc: 0.5398 - val_f1macro: 0.5196\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 48s 120ms/step - loss: 0.1748 - acc: 0.8299 - f1macro: 0.8210 - val_loss: 0.5146 - val_acc: 0.4812 - val_f1macro: 0.4691\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 48s 121ms/step - loss: 0.1700 - acc: 0.8347 - f1macro: 0.8266 - val_loss: 0.5051 - val_acc: 0.4930 - val_f1macro: 0.4768\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 58s 145ms/step - loss: 0.1694 - acc: 0.8345 - f1macro: 0.8268 - val_loss: 0.4995 - val_acc: 0.5047 - val_f1macro: 0.4840\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 56s 140ms/step - loss: 0.1678 - acc: 0.8361 - f1macro: 0.8282 - val_loss: 0.4735 - val_acc: 0.5336 - val_f1macro: 0.5083\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 48s 121ms/step - loss: 0.1624 - acc: 0.8412 - f1macro: 0.8335 - val_loss: 0.4953 - val_acc: 0.5039 - val_f1macro: 0.4886\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 51s 127ms/step - loss: 0.1636 - acc: 0.8397 - f1macro: 0.8325 - val_loss: 0.4791 - val_acc: 0.5234 - val_f1macro: 0.5058\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 48s 119ms/step - loss: 0.1647 - acc: 0.8385 - f1macro: 0.8312 - val_loss: 0.4611 - val_acc: 0.5391 - val_f1macro: 0.5107\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 49s 124ms/step - loss: 0.1614 - acc: 0.8416 - f1macro: 0.8346 - val_loss: 0.4837 - val_acc: 0.5211 - val_f1macro: 0.4905\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 51s 128ms/step - loss: 0.1590 - acc: 0.8444 - f1macro: 0.8372 - val_loss: 0.4537 - val_acc: 0.5437 - val_f1macro: 0.5164\n",
      "accuracy: 0.5317073170731708\n",
      "MAE: 0.4682926829268293\n",
      "F1: 0.664804469273743\n"
     ]
    }
   ],
   "source": [
    "DATADIR = \"Dataset\"\n",
    "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
    "TRAIN_VALID_RATIO = 0.75\n",
    "\n",
    "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "# to implement F1 score for validation in a batch\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1macro(y_true, y_pred):\n",
    "    f_pos = f1_m(y_true, y_pred)\n",
    "    # negative version of the data and prediction\n",
    "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
    "    return (f_pos + f_neg)/2\n",
    "\n",
    "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
    "    \"2D-CNNpred model according to the paper\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_len, n_features, 1)),\n",
    "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
    "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Flatten(),\n",
    "        Dropout(droprate),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
    "    \"As a generator to produce samples for Keras model\"\n",
    "    batch = []\n",
    "    while True:\n",
    "        # Pick one dataframe from the pool\n",
    "        key = random.choice(list(data.keys()))\n",
    "        df = data[key]\n",
    "        input_cols = [c for c in df.columns if c != targetcol]\n",
    "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
    "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
    "        assert split > seq_len, \"Training data too small for sequence length {}\".format(seq_len)\n",
    "        if kind == 'train':\n",
    "            index = index[:split]   # range for the training set\n",
    "        elif kind == 'valid':\n",
    "            index = index[split:]   # range for the validation set\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # Pick one position, then clip a sequence length\n",
    "        while True:\n",
    "            t = random.choice(index)     # pick one time step\n",
    "            n = (df.index == t).argmax() # find its position in the dataframe\n",
    "            if n-seq_len+1 < 0:\n",
    "                continue # this sample is not enough for one sequence length\n",
    "            frame = df.iloc[n-seq_len+1:n+1]\n",
    "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
    "            break\n",
    "        # if we get enough for a batch, dispatch\n",
    "        if len(batch) == batch_size:\n",
    "            X, y = zip(*batch)\n",
    "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
    "            yield X, y\n",
    "            batch = []\n",
    "\n",
    "def testgen(data, seq_len, targetcol):\n",
    "    \"Return array of all test samples\"\n",
    "    batch = []\n",
    "    for key, df in data.items():\n",
    "        input_cols = [c for c in df.columns if c != targetcol]\n",
    "        # find the start of test sample\n",
    "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
    "        n = (df.index == t).argmax()\n",
    "        # extract sample using a sliding window\n",
    "        for i in range(n+1, len(df)+1):\n",
    "            frame = df.iloc[i-seq_len:i]\n",
    "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
    "    X, y = zip(*batch)\n",
    "    return np.expand_dims(np.array(X),3), np.array(y)\n",
    "\n",
    "# Read data into pandas DataFrames\n",
    "data = {}\n",
    "for filename in os.listdir(DATADIR):\n",
    "    if not filename.lower().endswith(\".csv\"):\n",
    "        continue # read only the CSV files\n",
    "    filepath = os.path.join(DATADIR, filename)\n",
    "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
    "    # basic preprocessing: get the name, the classification\n",
    "    # Save the target variable as a column in dataframe for easier dropna()\n",
    "    name = X[\"Name\"][0]\n",
    "    del X[\"Name\"]\n",
    "    cols = X.columns\n",
    "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
    "    X.dropna(inplace=True)\n",
    "    # Fit the standard scaler using the training dataset\n",
    "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
    "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
    "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
    "    # Save scale transformed dataframe\n",
    "    X[cols] = scaler.transform(X[cols])\n",
    "    data[name] = X\n",
    "\n",
    "seq_len = 60\n",
    "batch_size = 128\n",
    "n_epochs = 20\n",
    "n_features = 82\n",
    "\n",
    "# Produce CNNpred as a binary classification problem\n",
    "model = cnnpred_2d(seq_len, n_features)\n",
    "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
    "model.summary()  # print model structure to console\n",
    "\n",
    "# Set up callbacks and fit the model\n",
    "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
    "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
    "callbacks = [\n",
    "    ModelCheckpoint(checkpoint_path,\n",
    "                    monitor='val_f1macro', mode=\"max\",\n",
    "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
    "]\n",
    "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
    "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
    "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)\n",
    "\n",
    "# Prepare test data\n",
    "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
    "\n",
    "# Test the model\n",
    "test_out = model.predict(test_data)\n",
    "test_pred = (test_out > 0.5).astype(int)\n",
    "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
    "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
    "print(\"F1:\", f1_score(test_pred, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5438585-b8e5-4b36-99d3-ac78e640ae56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 1.79308848],\n",
       "         [ 0.0434542 ],\n",
       "         [ 1.11494534],\n",
       "         ...,\n",
       "         [-0.20255338],\n",
       "         [ 0.19855601],\n",
       "         [-0.12686326]],\n",
       "\n",
       "        [[ 1.79371447],\n",
       "         [ 0.06469912],\n",
       "         [-0.8152429 ],\n",
       "         ...,\n",
       "         [-0.6530362 ],\n",
       "         [-0.2553898 ],\n",
       "         [-0.60005494]],\n",
       "\n",
       "        [[ 1.57248736],\n",
       "         [-1.06454673],\n",
       "         [-0.01233412],\n",
       "         ...,\n",
       "         [ 0.33903833],\n",
       "         [ 0.38112117],\n",
       "         [ 0.30082922]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.01718834],\n",
       "         [ 0.02898141],\n",
       "         [ 0.23150623],\n",
       "         ...,\n",
       "         [ 1.40197532],\n",
       "         [ 2.18210184],\n",
       "         [ 1.44740906]],\n",
       "\n",
       "        [[ 2.03839128],\n",
       "         [-0.34372459],\n",
       "         [ 0.18870716],\n",
       "         ...,\n",
       "         [ 1.83221172],\n",
       "         [ 0.08506956],\n",
       "         [-0.42715798]],\n",
       "\n",
       "        [[ 1.9818682 ],\n",
       "         [ 0.01828479],\n",
       "         [-0.77912276],\n",
       "         ...,\n",
       "         [-0.70871385],\n",
       "         [ 0.14427988],\n",
       "         [ 0.40092746]]],\n",
       "\n",
       "\n",
       "       [[[ 1.79371447],\n",
       "         [ 0.06469912],\n",
       "         [-0.8152429 ],\n",
       "         ...,\n",
       "         [-0.6530362 ],\n",
       "         [-0.2553898 ],\n",
       "         [-0.60005494]],\n",
       "\n",
       "        [[ 1.57248736],\n",
       "         [-1.06454673],\n",
       "         [-0.01233412],\n",
       "         ...,\n",
       "         [ 0.33903833],\n",
       "         [ 0.38112117],\n",
       "         [ 0.30082922]],\n",
       "\n",
       "        [[ 1.44719226],\n",
       "         [ 0.0347365 ],\n",
       "         [-1.72010437],\n",
       "         ...,\n",
       "         [ 0.14669735],\n",
       "         [ 0.1294773 ],\n",
       "         [ 1.46560874]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.03839128],\n",
       "         [-0.34372459],\n",
       "         [ 0.18870716],\n",
       "         ...,\n",
       "         [ 1.83221172],\n",
       "         [ 0.08506956],\n",
       "         [-0.42715798]],\n",
       "\n",
       "        [[ 1.9818682 ],\n",
       "         [ 0.01828479],\n",
       "         [-0.77912276],\n",
       "         ...,\n",
       "         [-0.70871385],\n",
       "         [ 0.14427988],\n",
       "         [ 0.40092746]],\n",
       "\n",
       "        [[ 1.98574446],\n",
       "         [ 0.0280446 ],\n",
       "         [ 0.00565335],\n",
       "         ...,\n",
       "         [ 1.22988076],\n",
       "         [ 0.51927859],\n",
       "         [ 0.4555265 ]]],\n",
       "\n",
       "\n",
       "       [[[ 1.57248736],\n",
       "         [-1.06454673],\n",
       "         [-0.01233412],\n",
       "         ...,\n",
       "         [ 0.33903833],\n",
       "         [ 0.38112117],\n",
       "         [ 0.30082922]],\n",
       "\n",
       "        [[ 1.44719226],\n",
       "         [ 0.0347365 ],\n",
       "         [-1.72010437],\n",
       "         ...,\n",
       "         [ 0.14669735],\n",
       "         [ 0.1294773 ],\n",
       "         [ 1.46560874]],\n",
       "\n",
       "        [[ 1.25220122],\n",
       "         [-0.00714082],\n",
       "         [-2.67294354],\n",
       "         ...,\n",
       "         [ 0.637673  ],\n",
       "         [ 1.08177666],\n",
       "         [ 1.2745121 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.9818682 ],\n",
       "         [ 0.01828479],\n",
       "         [-0.77912276],\n",
       "         ...,\n",
       "         [-0.70871385],\n",
       "         [ 0.14427988],\n",
       "         [ 0.40092746]],\n",
       "\n",
       "        [[ 1.98574446],\n",
       "         [ 0.0280446 ],\n",
       "         [ 0.00565335],\n",
       "         ...,\n",
       "         [ 1.22988076],\n",
       "         [ 0.51927859],\n",
       "         [ 0.4555265 ]],\n",
       "\n",
       "        [[ 2.0112012 ],\n",
       "         [ 0.01007093],\n",
       "         [ 0.24288239],\n",
       "         ...,\n",
       "         [-0.56192732],\n",
       "         [ 0.22816117],\n",
       "         [ 0.23713034]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 3.04411982],\n",
       "         [-0.2386711 ],\n",
       "         [ 0.00575657],\n",
       "         ...,\n",
       "         [-0.41514078],\n",
       "         [-0.91657175],\n",
       "         [-0.46355734]],\n",
       "\n",
       "        [[ 3.07244711],\n",
       "         [-0.45241338],\n",
       "         [ 0.26054257],\n",
       "         ...,\n",
       "         [-0.18736857],\n",
       "         [ 0.34658182],\n",
       "         [ 0.32812874]],\n",
       "\n",
       "        [[ 3.07563485],\n",
       "         [-0.49779316],\n",
       "         [-0.03895762],\n",
       "         ...,\n",
       "         [-1.28067519],\n",
       "         [-0.26525819],\n",
       "         [ 0.00963434]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.44411065],\n",
       "         [-0.1793864 ],\n",
       "         [-0.11432102],\n",
       "         ...,\n",
       "         [ 0.3137303 ],\n",
       "         [ 1.32848634],\n",
       "         [ 1.0470161 ]],\n",
       "\n",
       "        [[ 3.31288108],\n",
       "         [ 0.26183796],\n",
       "         [-1.5706072 ],\n",
       "         ...,\n",
       "         [-2.13608638],\n",
       "         [ 0.62289666],\n",
       "         [ 0.81042026]],\n",
       "\n",
       "        [[ 3.42245318],\n",
       "         [-0.07548628],\n",
       "         [ 0.06976409],\n",
       "         ...,\n",
       "         [-1.25536716],\n",
       "         [ 1.29888118],\n",
       "         [ 0.76492106]]],\n",
       "\n",
       "\n",
       "       [[[ 3.07244711],\n",
       "         [-0.45241338],\n",
       "         [ 0.26054257],\n",
       "         ...,\n",
       "         [-0.18736857],\n",
       "         [ 0.34658182],\n",
       "         [ 0.32812874]],\n",
       "\n",
       "        [[ 3.07563485],\n",
       "         [-0.49779316],\n",
       "         [-0.03895762],\n",
       "         ...,\n",
       "         [-1.28067519],\n",
       "         [-0.26525819],\n",
       "         [ 0.00963434]],\n",
       "\n",
       "        [[ 3.04624468],\n",
       "         [-0.08201318],\n",
       "         [-0.42552254],\n",
       "         ...,\n",
       "         [ 0.04040365],\n",
       "         [-0.03335109],\n",
       "         [-0.32705974]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.31288108],\n",
       "         [ 0.26183796],\n",
       "         [-1.5706072 ],\n",
       "         ...,\n",
       "         [-2.13608638],\n",
       "         [ 0.62289666],\n",
       "         [ 0.81042026]],\n",
       "\n",
       "        [[ 3.42245318],\n",
       "         [-0.07548628],\n",
       "         [ 0.06976409],\n",
       "         ...,\n",
       "         [-1.25536716],\n",
       "         [ 1.29888118],\n",
       "         [ 0.76492106]],\n",
       "\n",
       "        [[ 3.28283153],\n",
       "         [ 0.13375614],\n",
       "         [-1.66994152],\n",
       "         ...,\n",
       "         [-0.62266657],\n",
       "         [-0.12216658],\n",
       "         [ 0.43732682]]],\n",
       "\n",
       "\n",
       "       [[[ 3.07563485],\n",
       "         [-0.49779316],\n",
       "         [-0.03895762],\n",
       "         ...,\n",
       "         [-1.28067519],\n",
       "         [-0.26525819],\n",
       "         [ 0.00963434]],\n",
       "\n",
       "        [[ 3.04624468],\n",
       "         [-0.08201318],\n",
       "         [-0.42552254],\n",
       "         ...,\n",
       "         [ 0.04040365],\n",
       "         [-0.03335109],\n",
       "         [-0.32705974]],\n",
       "\n",
       "        [[ 3.11884064],\n",
       "         [ 0.83365343],\n",
       "         [ 0.78750773],\n",
       "         ...,\n",
       "         [ 0.59718017],\n",
       "         [ 1.21993408],\n",
       "         [ 1.58390666]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.42245318],\n",
       "         [-0.07548628],\n",
       "         [ 0.06976409],\n",
       "         ...,\n",
       "         [-1.25536716],\n",
       "         [ 1.29888118],\n",
       "         [ 0.76492106]],\n",
       "\n",
       "        [[ 3.28283153],\n",
       "         [ 0.13375614],\n",
       "         [-1.66994152],\n",
       "         ...,\n",
       "         [-0.62266657],\n",
       "         [-0.12216658],\n",
       "         [ 0.43732682]],\n",
       "\n",
       "        [[ 3.36528505],\n",
       "         [-0.149652  ],\n",
       "         [ 0.94912706],\n",
       "         ...,\n",
       "         [-0.85550039],\n",
       "         [-0.16164013],\n",
       "         [-0.5454559 ]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26152e20-b294-4dde-9580-893f5431afc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
